{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing benchmark log to pandas dataframe for the following benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.baseline_analyzer.BenchmarkAnalyzer import BenchmarkAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_parse_list = []\n",
    "\n",
    "to_parse_list.append('baseline_run0')\n",
    "to_parse_list.append('baseline_run1')\n",
    "to_parse_list.append('baseline_run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing baseline_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "12000\n",
      "Saved benchmark to ./results/baseline_run0.pkl\n",
      "Parsing baseline_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "12000\n",
      "Saved benchmark to ./results/baseline_run1.pkl\n",
      "Parsing baseline_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "12000\n",
      "Saved benchmark to ./results/baseline_run2.pkl\n"
     ]
    }
   ],
   "source": [
    "FOLDER_NAME = './results/'\n",
    "\n",
    "for benchmark_to_parse in to_parse_list:\n",
    "    print('Parsing {}'.format(benchmark_to_parse))\n",
    "    subprocess.check_output(['tar','-xzf', FOLDER_NAME + benchmark_to_parse + '.tar.gz', '-C', FOLDER_NAME])\n",
    "    log_file = FOLDER_NAME + benchmark_to_parse + '/'\n",
    "    my_analyzer = BenchmarkAnalyzer(log_folder=log_file)\n",
    "    my_df = my_analyzer.get_result_df()\n",
    "\n",
    "    instance_list = ['q0000']\n",
    "    my_df[my_df.Instance.isin(instance_list)].sort_values('Instance')\n",
    "    print(len(my_df))\n",
    "    my_df.to_pickle(FOLDER_NAME + benchmark_to_parse + '.pkl')\n",
    "    print('Saved benchmark to ' + FOLDER_NAME + benchmark_to_parse + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.naive_analyzer.BenchmarkAnalyzer import BenchmarkAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_parse_list = []\n",
    "\n",
    "to_parse_list.append('naive_run0')\n",
    "to_parse_list.append('naive_run1')\n",
    "to_parse_list.append('naive_run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing naive_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/naive_run0.pkl\n",
      "Parsing naive_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/naive_run1.pkl\n",
      "Parsing naive_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/naive_run2.pkl\n"
     ]
    }
   ],
   "source": [
    "FOLDER_NAME = './results/'\n",
    "\n",
    "for benchmark_to_parse in to_parse_list:\n",
    "    print('Parsing {}'.format(benchmark_to_parse))\n",
    "    subprocess.check_output(['tar','-xzf', FOLDER_NAME + benchmark_to_parse + '.tar.gz', '-C', FOLDER_NAME])\n",
    "    log_file = FOLDER_NAME + benchmark_to_parse + '/'\n",
    "    my_analyzer = BenchmarkAnalyzer(log_folder=log_file)\n",
    "    my_df = my_analyzer.get_result_df()\n",
    "    \n",
    "    instance_list = ['q0000']\n",
    "    my_df[my_df.Instance.isin(instance_list)].sort_values('Instance')\n",
    "#     my_df.to_pickle('../experiment_pipeline/model/Jan27_q10000_LeNet_mockup_JSSP_Only/' + benchmark_to_parse + '.pkl')\n",
    "#     print('Saved benchmark to ' + '../experiment_pipeline/model/Jan27_q10000_LeNet_mockup_JSSP_Only/' + benchmark_to_parse + '.pkl')\n",
    "    my_df.to_pickle(FOLDER_NAME + benchmark_to_parse + '.pkl')\n",
    "    print('Saved benchmark to ' + FOLDER_NAME + benchmark_to_parse + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.BenchmarkAnalyzer import BenchmarkAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_parse_list = []\n",
    "\n",
    "to_parse_list.append('hybrid_jump_half_exhaustive_run0')\n",
    "to_parse_list.append('hybrid_jump_half_exhaustive_run1')\n",
    "to_parse_list.append('hybrid_jump_half_exhaustive_run2')\n",
    "\n",
    "to_parse_list.append('hybrid_jump_half_naive_run0')\n",
    "to_parse_list.append('hybrid_jump_half_naive_run1')\n",
    "to_parse_list.append('hybrid_jump_half_naive_run2')\n",
    "\n",
    "to_parse_list.append('hybrid_jump_std_exhaustive_run0')\n",
    "to_parse_list.append('hybrid_jump_std_exhaustive_run1')\n",
    "to_parse_list.append('hybrid_jump_std_exhaustive_run2')\n",
    "\n",
    "to_parse_list.append('hybrid_jump_3std_exhaustive_run0')\n",
    "to_parse_list.append('hybrid_jump_3std_exhaustive_run1')\n",
    "to_parse_list.append('hybrid_jump_3std_exhaustive_run2')\n",
    "\n",
    "to_parse_list.append('hybrid_jump_5std_exhaustive_run0')\n",
    "to_parse_list.append('hybrid_jump_5std_exhaustive_run1')\n",
    "to_parse_list.append('hybrid_jump_5std_exhaustive_run2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing hybrid_jump_half_exhaustive_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_half_exhaustive_run0.pkl\n",
      "Parsing hybrid_jump_half_exhaustive_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_half_exhaustive_run1.pkl\n",
      "Parsing hybrid_jump_half_exhaustive_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_half_exhaustive_run2.pkl\n",
      "Parsing hybrid_jump_half_naive_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_half_naive_run0.pkl\n",
      "Parsing hybrid_jump_half_naive_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_half_naive_run1.pkl\n",
      "Parsing hybrid_jump_half_naive_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_half_naive_run2.pkl\n",
      "Parsing hybrid_jump_std_exhaustive_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_std_exhaustive_run0.pkl\n",
      "Parsing hybrid_jump_std_exhaustive_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_std_exhaustive_run1.pkl\n",
      "Parsing hybrid_jump_std_exhaustive_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_std_exhaustive_run2.pkl\n",
      "Parsing hybrid_jump_3std_exhaustive_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_3std_exhaustive_run0.pkl\n",
      "Parsing hybrid_jump_3std_exhaustive_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_3std_exhaustive_run1.pkl\n",
      "Parsing hybrid_jump_3std_exhaustive_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_3std_exhaustive_run2.pkl\n",
      "Parsing hybrid_jump_5std_exhaustive_run0\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_5std_exhaustive_run0.pkl\n",
      "Parsing hybrid_jump_5std_exhaustive_run1\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_5std_exhaustive_run1.pkl\n",
      "Parsing hybrid_jump_5std_exhaustive_run2\n",
      "Finish reading 12000 benchmarking logs.\n",
      "Finish parsing the logs\n",
      "Finish constructing pandas dataframe.\n",
      "Saved benchmark to ./results/hybrid_jump_5std_exhaustive_run2.pkl\n"
     ]
    }
   ],
   "source": [
    "FOLDER_NAME = './results/'\n",
    "\n",
    "for benchmark_to_parse in to_parse_list:\n",
    "    print('Parsing {}'.format(benchmark_to_parse))\n",
    "    subprocess.check_output(['tar','-xzf', FOLDER_NAME + benchmark_to_parse + '.tar.gz', '-C', FOLDER_NAME])\n",
    "    log_file = FOLDER_NAME + benchmark_to_parse + '/'\n",
    "    my_analyzer = BenchmarkAnalyzer(log_folder=log_file)\n",
    "    my_df = my_analyzer.get_result_df()\n",
    "    \n",
    "    instance_list = ['q0000']\n",
    "    my_df[my_df.Instance.isin(instance_list)].sort_values('Instance')\n",
    "    my_df.to_pickle(FOLDER_NAME + benchmark_to_parse + '.pkl')\n",
    "    print('Saved benchmark to ' + FOLDER_NAME + benchmark_to_parse + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
